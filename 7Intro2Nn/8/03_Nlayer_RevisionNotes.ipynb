{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "onTb6K8GDcA5",
        "9OBG5-3qEBzy",
        "miLPyASltzx9",
        "rBy7Ro-wMVPk",
        "KQJGCJYeMrvp",
        "Ip73Rut6PvmR",
        "Rtjvq23hqOAR",
        "mQi26y-Adx8h",
        "922BK2gpmHEN",
        "pRtHGWr3lJU9",
        "_Bg08QrImyzP",
        "mPKWUXO4Gvy9",
        "qzxqJcCHipyw",
        "cLp_JIjxrU-A",
        "hlxH5PBZr-gQ",
        "5eIlR72-q5Kq",
        "KPgBL3nStazI",
        "ZxW1jv5aq5Kq",
        "vJK4rGSHw6_o",
        "7MR8e9k6q5Kq",
        "y1C7qTOrq5Kq",
        "F4bUrVu5q5Kr",
        "NZnWy-zrcQ2y",
        "8BY45N9trEWD",
        "eG79j4RS0j4z"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "\n",
        "Lecture 1 : https://www.scaler.com/meetings/i/nn-n-layer-nn-contd/archive\n",
        "\n",
        "Lecture 2 : https://www.scaler.com/meetings/i/nn-backprop-for-n-layer-nns/archive"
      ],
      "metadata": {
        "id": "XWjpSr4ZvMhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "- **Recap**\n",
        "\n",
        "- **MLP: Multi layer Preceptron**\n",
        "\n",
        "- **Activation Functions**\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "- **Notations**\n",
        "    \n",
        "\n",
        "\n",
        "- **Forward Propagation**\n",
        "    \n"
      ],
      "metadata": {
        "id": "1QkOqtrdAyo3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recap"
      ],
      "metadata": {
        "id": "xF8uZE9SWkcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We made a simple Neural network to classify a  multiclass setting use case"
      ],
      "metadata": {
        "id": "FvAGvcyxXR8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1-f3HjcdZ_K4SeT3P8pr7Bp_zFjJixXXi' width=\"800\"></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QoOEHBNMtqlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Revision) Question: How many weights did we have for 2 feature input and 3 neurons?\n",
        "\n",
        "Ans: Since we have 2 input and 3 neuron, we have total of 2*3 = 6 weights and 3 bias (one per neuron.)"
      ],
      "metadata": {
        "id": "onTb6K8GDcA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Revision) What activation did we use in output ?\n",
        "\n",
        "Ans: Softmax"
      ],
      "metadata": {
        "id": "9OBG5-3qEBzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we only had a simple network of 3 neuron.\n",
        "\n",
        "#### What if we add more neuron to our network and make it more complex ?\n",
        "\n",
        "Let's see how we do it"
      ],
      "metadata": {
        "id": "miLPyASltzx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP: Multi Layer Perceptron"
      ],
      "metadata": {
        "id": "Nm1EUxZdDsTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We make our model/ network complex by adding a layer between our input and output."
      ],
      "metadata": {
        "id": "6SIvTk5_Lmop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1wMWkvsARzV1eN0JpUcgg1J63dJkkcUAD' width=\"800\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "e4kQZtezLt0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's understand what we did here.\n",
        "\n",
        "#### What are we given with?\n",
        "\n",
        "- We know that we have 2 feature input and\n",
        "- we have 3 classes in output.\n",
        "\n"
      ],
      "metadata": {
        "id": "rBy7Ro-wMVPk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Understanding the architecture\n",
        "$ $\n",
        "- Here, we use superscript notation to represent the layer.\n",
        "    - So, instead of writing $f_1$, we write it as $f^1_1$ i.e. neuron 1 of layer 1\n",
        "\n",
        "<br>\n",
        "\n",
        "Now,\n",
        "\n",
        "We took our 2 inputs (Layer - 0 / **Input Layer**) and connected them with the the 4 neuron of **Layer-1**\n",
        "- These 4 neurons will be connected with the 3 neuron of the **output layer** (Layer -2).\n",
        "\n",
        "This intermediate layer in between the input and output layer is called **Hidden layer**\n",
        "\n"
      ],
      "metadata": {
        "id": "KQJGCJYeMrvp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### But, why is called hidden layer?\n",
        "\n",
        "As both input and output are hidden from this layer,\n",
        "and\n",
        "- we don't directly deal with this layer,\n",
        "- hence, it is called hidden layer.\n",
        "\n",
        "We make our network complex by increasing the number of neuron in the layer or by increasing the number of layers."
      ],
      "metadata": {
        "id": "Ip73Rut6PvmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1G1J430m73eqrBQ0NVCVCHspjaPMpgEba' width=\"800\"></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "-1geLbxnU3b5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: What will be the activation function for output layer?\n",
        "\n",
        "Ans: Since it is a multiclass classification problem, we'll use softmax activation function"
      ],
      "metadata": {
        "id": "Rtjvq23hqOAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What will be the activation function for hidden layer ?\n",
        "\n",
        "Recall that we want our model to learn non linear decision boundary,\n",
        "- Hence, we'll always use non linear activation function\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mQi26y-Adx8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: What happens if each activation function in MLP is a linear function?"
      ],
      "metadata": {
        "id": "922BK2gpmHEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to understand it using an example"
      ],
      "metadata": {
        "id": "W9W8CkSKk_nJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRtHGWr3lJU9"
      },
      "source": [
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Case 1:**\n",
        "\n",
        "Given $f(x) = 2x+1, g(x) = 3x+2$, will $f(g(x))$ be linear or non-linear?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1TNF_X-rg9Wee84yURhbc8AfAE0T758f4' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "y4m-x1Y1GkeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(g(x)) = 2(3x+2) + 1 = 6x + 5$ ---> linear\n",
        "\n",
        "- Composition of two linear function is linear.\n",
        "\n",
        "If we use linear activation\n",
        "- it'll become a linear model.\n",
        "\n",
        "Remember, we have to generare higher order non-linear features\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "A4J-JX1JGhCR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OdLM2mIajSCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### How non-linear function helps us here ?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Bg08QrImyzP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Case 2:** $f(x) = x^2+1, g(x) = 2x+1$\n",
        "\n"
      ],
      "metadata": {
        "id": "PGSm2cBJGzfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1hZsaEgqvksWKed5pcAtNeGpEWTMexZTA' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "0iLAP0rBHirX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$f(g(x)) = (2x+1)^2 + 1 = 4x^2 + 2x + 2$ ---> non-linear, higher order\n",
        "\n",
        "<br>"
      ],
      "metadata": {
        "id": "l3nTM6vzHhWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q. How about we stack non-linearity once more?\n",
        "\n"
      ],
      "metadata": {
        "id": "mPKWUXO4Gvy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Case 3:** $h(x) = x^2+1, f(x) = 2x^2+1, g(x) = x+1$\n"
      ],
      "metadata": {
        "id": "_1fjW79TIxas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1IDG5hLnzrQvKUE0PKgaxr-6_2vcqrWn7' width=\"800\"></center>\n"
      ],
      "metadata": {
        "id": "IexqaIlRJP6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$f(h(g(x))) = 2((x+1)^2+1)^2 + 1 =  2(2x^2 + 2x + 3)^2 + 1$ ---> non-linear, much higher order\n",
        "\n",
        "<br>\n",
        "\n",
        "Conclusion - **Stacking a non-linearity over a linear function, and repeating the process may create complex features**"
      ],
      "metadata": {
        "id": "DNj8pV4eJOpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example, suppose we have 2 inputs: $x_1, x_2$\n",
        "\n",
        "They are going through the following **computation graph**.\n",
        "\n",
        "  <center><img src='https://drive.google.com/uc?id=1zVmAdtYjnAiZ0trhE2XV9pSrOw9fFOF9' width=800></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "0gtJg-c_EUNr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M57VBrYT0CF7"
      },
      "source": [
        "- $z$ is a weighted sum, so it'll be linear only: $z = 3x_1 + 4x_2$\n",
        "- $a_1$ will be non-linear and complex: $a_1 = 9x_1^2 + 16x_2^2 + 24x_1x_2$\n",
        "- $a_2$ will be non-linear and much more complex: $a_2 = (9x_1^2 + 16x_2^2 + 24x_1x_2 +1)^2 = (9x_1^2 + 16x_2^2)^2 + 2(9x_1^2 + 16x_2^2)(24x_1x_2 +1) + (24x_1x_2 +1)^2$\n",
        "\n",
        "<hr style=\"border:1px solid gray\"> </hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What non-linear function shall we use?  Have you seen any non linear function in the ML algos so far ?\n",
        "\n",
        "\n",
        "Ans: Recall Sigmoid is one of the non linear activation function we saw.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qzxqJcCHipyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: Should we use different activation functions in different layers?\n",
        "\n",
        "***Anwser:***\n",
        "- Theoretically we can. But studies have shown that it is not of much value.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cLp_JIjxrU-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation function"
      ],
      "metadata": {
        "id": "bG9aCo9Tiq8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We saw that how sigmoid can be used as a activation function for hidden layer."
      ],
      "metadata": {
        "id": "H6520VEBq6o7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do note that\n",
        "- Domain of sigmoid is $(-âˆž, âˆž)$\n",
        "- Range is (0,1)\n",
        "- Derivative of sigmoid also lies between (0,1)\n",
        "\n",
        "Also, derivative of sigmoid can also be represented in terms of sigmoid."
      ],
      "metadata": {
        "id": "bxgYc7eJX6kv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, Sigmoid activation function was very popular in 1980's and 1990's"
      ],
      "metadata": {
        "id": "QR9hrhLztjRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?id=1YsuzkFwJUxGkAtjV8_QYnui4Sx_kqurC'>"
      ],
      "metadata": {
        "id": "saagnU2hs4oT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Are there any other activation functions ?"
      ],
      "metadata": {
        "id": "hlxH5PBZr-gQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### tanh function\n",
        "\n",
        "- Shifted version of sigmoid function\n",
        "- Works better than sigmoid almost all the time - mean value is zero\n",
        "- Inputs lies in the range: $(- âˆž, âˆž)$\n",
        "- Output lies in the range: $(-1, 1)$\n",
        "- We don't use tanh function very often, unless we want output to lie in the range of (-1, 1).\n",
        "- Formula: $tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
        "\n",
        "- Derivative of tanh function: $\\frac{d(tanh(z))}{dz} = 1-tanh^2(z)$\n",
        " - This lies between $(0, 1)$\n",
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1M2i6y-FQd9mmsxK1jNPyVicfe3ETZvq5)"
      ],
      "metadata": {
        "id": "5eIlR72-q5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Issue with Sigmoid and tanh"
      ],
      "metadata": {
        "id": "KPgBL3nStazI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Vanishing Gradients"
      ],
      "metadata": {
        "id": "ZxW1jv5aq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Downside of both sigmoid and tanh is that their gradient is <1, for most of the values of z**\n",
        "\n",
        "- This hampers the gradient descent process, and the calculated gradients will be very small.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zfFRGjWGq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why does small gradient hampers GD process ?"
      ],
      "metadata": {
        "id": "yPV9Tt5cnq7U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Since the activation functions are sigmoid or tanh\n",
        "- We know that derivative of these functions lie between (0, 1).\n",
        "- So, the product of these terms inside the bracket, will become very small.\n",
        "- In fact, as the number of layers in the NN increase, this product will become smaller and smaller.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FdwxIBrGq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1WBZQqwBCAUFDCCcdliaSlKDDKs5ypw-G)\n",
        "\n"
      ],
      "metadata": {
        "id": "SLFuNcCAwdpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The equation to update the weight $w$ is: $w_{new} = w_{{old}} - ð¶.\\frac{âˆ‚ \\ loss}{âˆ‚ \\ w} \\biggr\\vert_{{W}_{old}}$\n",
        "\n",
        "- We just saw that this partial derivative value becomes miniscule, as the number of layers increase.\n",
        "- As a result, the NN gets trained very very slowly.\n",
        "- In fact, for close to 2 decades, people could not imagine using a NN with more than 3-4 layers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JrTPBP3Qq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1C4-Z6rDkRA-fzEaxelwruZfGD5Rii1uI\" width=600></center>\n",
        "\n",
        "<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "y07g_tMQUsfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How shall we deal with this problem ?  "
      ],
      "metadata": {
        "id": "vJK4rGSHw6_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: What should be the properties of ideal activation function?"
      ],
      "metadata": {
        "id": "VE3nHB0oszpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should be:\n",
        "- differentiable\n",
        "- non linear\n",
        "- easy to calculate\n",
        "- gradient $â‰¥ 1$ for big range of z"
      ],
      "metadata": {
        "id": "o8pS7qDds34-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ReLu\n",
        "\n",
        "- Stands for **Rectified Linear Unit**.\n",
        "- It says, that for any positive value, it will return that value as it is: $Relu(z) = z$, if $z > 0$\n",
        "- Otherwise, it returns 0: $Relu(z) = 0$, if $z <= 0$\n",
        "\n",
        "It can be stated as: $Relu(z) = max(z, 0)$\n",
        "- Very fast to compute.\n",
        "\n",
        "- Derivative of ReLu wrt z: $ReLu'(z) = \\left\\{\\begin{matrix}\n",
        "1, if \\ z>0\\\\\n",
        "0, if \\ z<0\n",
        "\\end{matrix}\\right.$\n",
        "\n",
        " - Though it is not differential at 0, as it is not continuous\n",
        " - So here we take an approximation, for it to work. So we make it as:$ReLu'(z) = \\left\\{\\begin{matrix}\n",
        "1, if \\ z>0\\\\\n",
        "0, if \\ z<=0\n",
        "\\end{matrix}\\right.$\n",
        "\n"
      ],
      "metadata": {
        "id": "7MR8e9k6q5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=11WM7DXTHZNFI06flIdUcR-zKbn8n1YBz)"
      ],
      "metadata": {
        "id": "RvheEXX_t1QW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question: Is there a problem in practically using ReLu as activation function?\n",
        "\n",
        "Yes. Even though it is the most widely used activation function in the world of Deep Learning, there is a slight problem.\n",
        "\n",
        "\n",
        "- If even one derivative term in calculation of $\\frac{\\partial Loss}{âˆ‚ w}$ gets the value as 0, the entire term will become zero.\n",
        "\n",
        "- Hence, there is no update in the value of weight.\n",
        "- This is also know as **\"dying ReLU\"**\n",
        "- So there's a potential **vanishing gradient** problem.\n",
        "    - While calculating backprop, if one of the derivative is 0, the whole update will become zero and network won't update\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "However, to deal with this, a slight modification is made to ReLu, and we get another activation function known as **Leaky ReLu**\n",
        "\n"
      ],
      "metadata": {
        "id": "y1C7qTOrq5Kq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://drive.google.com/uc?export=view&id=1l4UnOtfTGexfm2gJEj4sTnHwAzzym1mR)"
      ],
      "metadata": {
        "id": "7DbC82VaxJay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Leaky ReLu\n",
        "\n",
        "- This is very similar to ReLu but there's a twist\n",
        "- In case of negative vales, we add a small gradient ($Î±$) associated with it, instead of having 0.\n",
        "- Gradient: $Leaky \\ ReLu'(z) = \\left\\{\\begin{matrix}\n",
        "1, if \\ z>0\\\\\n",
        "Î±, if \\ z<=0\n",
        "\\end{matrix}\\right.$\n",
        "\n"
      ],
      "metadata": {
        "id": "F4bUrVu5q5Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![picture](https://drive.google.com/uc?export=view&id=1pbkhu3NxintyCdHo5DSVy6M6aQaDhbcI)"
      ],
      "metadata": {
        "id": "1s67e4Bsq5Kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notations for N layer NN"
      ],
      "metadata": {
        "id": "RK-cZ904qRhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "So, we introduce layer number in notation as superscript\n",
        "\n",
        "Now,\n",
        "As layer increase, earlier weight notation will not work.\n",
        "- So, we add layer number to weight notation as well."
      ],
      "metadata": {
        "id": "dloOzLLyTN25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "For weight: $w^L_{ij}$\n",
        "\n",
        "where\n",
        "- L represent layer number\n",
        "- and i represents from neuron\n",
        "- j represent to neuron\n",
        "\n",
        "<br>\n",
        "\n",
        "Bias: $b^L_i$\n",
        "where\n",
        "- L represents the layer\n",
        "- i represents the neuron."
      ],
      "metadata": {
        "id": "04JInT7ySCcK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src='https://drive.google.com/uc?id=1LAnU_9qgGqf9LbK1PzotW6J80WRYb9Rx' width=\"800\"></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aX18RkNPbgOI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quiz\n",
        "\n",
        "```\n",
        "How many total parameters do we have for this network?\n",
        "\n",
        "a. 15\n",
        "b. 21\n",
        "c. 27\n",
        "d. 30\n",
        "\n",
        "\n",
        "```\n",
        "Ans: 27 (c)\n",
        "\n",
        "- For first layer,\n",
        "    - we have 2*4 = 8 weights and 4 biases i.e. 12 params\n",
        "\n",
        "- For second layer,\n",
        "    - we have 4*3 = 12 and 3 biases i.e. 15 params\n",
        "\n",
        "Hence, we have total of 12 + 15 = 27\n"
      ],
      "metadata": {
        "id": "NZnWy-zrcQ2y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BY45N9trEWD"
      },
      "source": [
        "\n",
        "\n",
        "We will now have two sets of weights and biases (for the first and second layers) - $W^1, W^2, b^1, b^2$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What will be the dimensions of the parameters $W^1, b^1, W^2, b^2$?\n",
        "\n",
        "\n",
        "Forget about the second layer for now, let's just focus on the first layer, and it's inputs.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### $W^1$\n",
        "- Here, we have **2 inputs** being passed to **4 neurons**\n",
        "- Recall that each pair of input and neuron, has a unique weights value.\n",
        "- Therefore, $W^1$ will be $2\\times4$ matrix:\n",
        "$W^1 = \\begin{bmatrix}\n",
        "w_{11}^1 & w_{12}^1 & w_{13}^1 & w_{14}^1\\\\\n",
        "w_{21}^1 & w_{22}^1 & w_{23}^1 & w_{24}^1\n",
        "\\end{bmatrix}_{2 \\times 4}$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### $b^1$\n",
        "- There are 4 neurons in the first layer.\n",
        "- Each of which has an unique bias value.\n",
        "- Therefore, $b^1$ will be $1 \\times 4$ matrix: $b^1 = \\begin{bmatrix}\n",
        "b_1^1 & b_2^1 & b_3^1 & b_4^1\n",
        "\\end{bmatrix}_{1 \\times 4}$\n"
      ],
      "metadata": {
        "id": "eG79j4RS0j4z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED3RYU5-2GUV"
      },
      "source": [
        "Let's figure dimensions $W^2$ and $b^2$ by considering only layer-1 and layer-2 again as a one layer NN\n",
        "\n",
        "\n",
        "#### $W^2$\n",
        "- Now, the outputs of the layer-1 are inputs to the layer-2.\n",
        "- Layer-2 neurons don't need to know if its an output from an earlier layer, or a raw input\n",
        "- Here, 4 inputs are passed to 3 neurons, $W^2$ will be $4\\times3$ matrix: $W^2 = \\begin{bmatrix}\n",
        "w_{11}^2 & w_{12}^2 & w_{13}^2\\\\\n",
        "w_{21}^2 & w_{22}^2 & w_{23}^2\\\\\n",
        "w_{31}^2 & w_{32}^2 & w_{33}^2\\\\\n",
        "w_{41}^2 & w_{42}^2 & w_{43}^2\n",
        "\\end{bmatrix}_{4 \\times 3}$\n",
        "\n",
        "<br>\n",
        "\n",
        "#### $b^2$\n",
        "- There are 3 neurons in second layer, each of which will have a bias associated with them.\n",
        "- Therefore, $b^2$ will be $1 \\times 3$ matrix: $b^2 = \\begin{bmatrix}\n",
        "b_1^2 & b_2^2 & b_3^2\n",
        "\\end{bmatrix}_{1 \\times 3}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets define a few variables, that define our NN"
      ],
      "metadata": {
        "id": "p0FXyVnn2-pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# initialize parameters randomly\n",
        "d = 2 # diensionality / number of inputs\n",
        "n = 3 # Number of classes (A/B/C) / Number of neurons in output layer\n",
        "h = 4 # neurons in hidden layer"
      ],
      "metadata": {
        "id": "UpSGuRU0y5oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let;s intitialise these matrices randomly."
      ],
      "metadata": {
        "id": "IHJReVDzyzwv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCY9Gn0lrEWE"
      },
      "outputs": [],
      "source": [
        "W1 = 0.01 * np.random.randn(d,h)\n",
        "b1 = np.zeros((1,h))\n",
        "W2 = 0.01 * np.random.randn(h,n)\n",
        "b2 = np.zeros((1,n))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have covered weights and biases, let's talk about forward propagation and backpropagation"
      ],
      "metadata": {
        "id": "F7NIZumEeFHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation"
      ],
      "metadata": {
        "id": "5oxI772Vep1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forward Propagation is all easy.\n",
        "- We just need to propagation our inputs from left to right\n",
        "- Calculate the value of $z_i$\n",
        "- Apply activation function on top of it\n",
        "- And pass it to neuron in front of it.\n",
        "\n",
        "Ultimately, we'll get the probabilities\n",
        "\n",
        "- Use those probabilities to calcualte the loss.\n",
        "\n",
        "\n",
        "#### Question: What will be the loss here?\n",
        "\n",
        "Ans: Since it is a multiclass classification => **Categorical cross entropy**"
      ],
      "metadata": {
        "id": "z-1htLcpeqUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, comes the tricky part."
      ],
      "metadata": {
        "id": "LMDaaAKbeqhE"
      }
    }
  ]
}