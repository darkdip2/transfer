pip install opencv-python numpy scikit-learn dlib face_recognition

import cv2
import face_recognition
import os

def extract_faces(image_path):
    image = cv2.imread(image_path)
    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    face_locations = face_recognition.face_locations(rgb_image)
    face_encodings = face_recognition.face_encodings(rgb_image, face_locations)

    faces = []
    for (top, right, bottom, left), encoding in zip(face_locations, face_encodings):
        face_crop = image[top:bottom, left:right]
        faces.append((face_crop, encoding))
    
    return faces



def process_gallery(folder_path):
    profiles = []
    
    for filename in os.listdir(folder_path):
        if filename.endswith(('.jpg', '.png', '.jpeg')):
            faces = extract_faces(os.path.join(folder_path, filename))
            for face, encoding in faces:
                profiles.append((filename, face, encoding))
    
    return profiles



from sklearn.cluster import DBSCAN
import numpy as np

def cluster_faces(profiles, eps=0.5):
    encodings = np.array([profile[2] for profile in profiles])
    
    clustering = DBSCAN(eps=eps, min_samples=2, metric="euclidean").fit(encodings)
    
    clusters = {}
    for i, label in enumerate(clustering.labels_):
        if label == -1:
            continue  # Ignore noise
        
        if label not in clusters:
            clusters[label] = []
        
        clusters[label].append(profiles[i])
    
    return clusters


def display_profiles(clusters):
    for label, profiles in clusters.items():
        print(f"Profile {label}:")
        for filename, face, _ in profiles:
            cv2.imshow(f"Profile {label}", face)
            cv2.waitKey(1000)
            cv2.destroyAllWindows()



folder_path = "gallery_images"
profiles = process_gallery(folder_path)
clusters = cluster_faces(profiles, eps=0.5)
display_profiles(clusters)






===============================================================================================================================================






pip install tensorflow keras-facenet opencv-python numpy scikit-learn face-recognition


from keras_facenet import FaceNet

# Load FaceNet model
embedder = FaceNet()



import os
import numpy as np
import cv2

known_faces = {}  # Dictionary to store names and embeddings

def register_known_faces(folder_path):
    for filename in os.listdir(folder_path):
        if filename.endswith(('.jpg', '.png', '.jpeg')):
            name = os.path.splitext(filename)[0]  # Extract name from file
            image_path = os.path.join(folder_path, filename)
            
            image = cv2.imread(image_path)
            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Generate embedding using FaceNet
            embedding = embedder.embeddings([image])[0]
            known_faces[name] = embedding

register_known_faces("known_faces")  # Folder with labeled images




from sklearn.metrics.pairwise import cosine_similarity

def recognize_face(embedding):
    best_match = None
    highest_similarity = 0.0

    for name, known_embedding in known_faces.items():
        similarity = cosine_similarity([embedding], [known_embedding])[0][0]

        if similarity > 0.5 and similarity > highest_similarity:  # Adjust threshold if needed
            highest_similarity = similarity
            best_match = name
    
    return best_match if best_match else "Unknown"




def cluster_and_recognize(profiles, eps=0.5):
    encodings = np.array([profile[2] for profile in profiles])
    
    clustering = DBSCAN(eps=eps, min_samples=2, metric="euclidean").fit(encodings)
    
    clusters = {}
    for i, label in enumerate(clustering.labels_):
        if label == -1:
            continue  # Ignore noise
        
        if label not in clusters:
            clusters[label] = []
        
        name = recognize_face(profiles[i][2])  # Identify name
        clusters[label].append((name, profiles[i][0], profiles[i][1]))
    
    return clusters



def display_named_profiles(clusters):
    for label, profiles in clusters.items():
        print(f"Profile {label} - {profiles[0][0]}:")
        for name, filename, face in profiles:
            cv2.imshow(f"{name} - Profile {label}", face)
            cv2.waitKey(1000)
            cv2.destroyAllWindows()



folder_path = "gallery_images"
profiles = process_gallery(folder_path)
clusters = cluster_and_recognize(profiles, eps=0.5)
display_named_profiles(clusters)



==========================================================================================================================================


pip install fastapi uvicorn opencv-python numpy moviepy python-multipart


import cv2
import numpy as np
import shutil
import uvicorn
from fastapi import FastAPI, UploadFile, File
from fastapi.responses import FileResponse
from ultralytics import YOLO  # Assuming YOLO is being used
import tempfile
import os

app = FastAPI()

# Load the object detection model
model = YOLO("yolov8n.pt")  # Change to your model

def process_video(input_path, output_path):
    cap = cv2.VideoCapture(input_path)
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        results = model(frame)

        for r in results:
            boxes = r.boxes
            for box in boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])
                conf = box.conf[0].item()
                cls = int(box.cls[0].item())

                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(frame, f"{model.names[cls]} {conf:.2f}", 
                            (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                            0.5, (0, 255, 0), 2)

        out.write(frame)

    cap.release()
    out.release()

@app.post("/upload_video/")
async def upload_video(file: UploadFile = File(...)):
    # Save uploaded file to a temporary directory
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_video:
        shutil.copyfileobj(file.file, temp_video)
        temp_video_path = temp_video.name

    output_video_path = temp_video_path.replace(".mp4", "_output.mp4")
    
    # Process video with object detection
    process_video(temp_video_path, output_video_path)

    # Return processed video
    return FileResponse(output_video_path, media_type="video/mp4", filename="processed_video.mp4")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)




Please complete the Assignment before 4 PM tomorrow and share it with me in PDF format.

Attached is a sample data file. 74368 rows of anonymized Data, set for the year 2052 for test purposes.

Question - 1

Test_Data_analysis.xlsx

Can you prepare an analysis on a google spreadsheet and answer the following questions through data views -

Which sleep phase (REM or Deep) shows higher variability across users? Please explain through data.
What is the average steps done - classified by sleep mid-point for different users.
How does stress levels vary based on early sleepers, late sleepers ( you can assume cutoff for early and late sleep.
Which weekday had most average sleep recorded.
Which user_id sleeps most and which user_id sleeps least.
Question-2

You are given a table named luna_daily_metrics with the following schema:

luna_daily_metrics (

user_id BIGINT,

date DATE, 

circadian_midpoint TIME, 

avg_heart_rate INTEGER,

master_avg_hrv INTEGER, 

master_duration INTEGER,
master_rem INTEGER, 

master_deep INTEGER, 

total_steps INTEGER, 

total_calories INTEGER, 

active_calories INTEGER, 

stress_value INTEGER, 

workout_count INTEGER 

)

Write a SQL query to identify the top 5 users with the highest average HRV over the past 7 days, only considering users who worked out at least 3 times in that period.














Comprehensive Guide to Deploying Machine Learning Models with Docker and AWSIntroductionDeploying machine learning (ML) models into production involves preparing a trained model, setting up a consistent environment, and ensuring scalability and reliability. This guide covers deploying ML models using PyTorch or TensorFlow, handling CPU and GPU environments, managing Python and module versions, setting up Docker images, installing CUDA, and deploying on AWS. It includes best practices to ensure reproducibility, performance, and scalability.1. Overview of ML Model DeploymentDeploying an ML model involves making a trained model available for inference (predictions) in a production environment. Key considerations include:Environment Consistency: Ensuring the deployment environment matches the training environment.Hardware Support: Supporting CPU and/or GPU for inference.Scalability: Handling varying request volumes.Portability: Using containers (e.g., Docker) for consistent deployments.Cloud Integration: Leveraging AWS for scalable infrastructure.This guide focuses on deploying models using Docker containers on AWS, with detailed steps for PyTorch and TensorFlow, including CPU/GPU handling and CUDA setup.2. Handling CPU and GPU EnvironmentsML models can run on CPUs or GPUs, with GPUs offering significant speedups for inference, especially for deep learning models. Hereâ€™s how to handle both:2.1 CPU DeploymentUse Case: Suitable for lightweight models or environments without GPUs.Advantages: Lower cost, easier setup, no need for GPU drivers or CUDA.Challenges: Slower inference for large models or high-throughput scenarios.Setup:Ensure your model is optimized for CPU (e.g., using ONNX or quantization).Use libraries like Intel MKL (Math Kernel Library) for PyTorch or TensorFlow to enhance CPU performance.Example: PyTorch CPU installation via pip:pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cpuVerify CPU usage in Python:import torch
print(torch.device("cpu"))  # Ensure model runs on CPU2.2 GPU DeploymentUse Case: Ideal for deep learning models requiring fast inference (e.g., neural networks).Advantages: Significant speed improvements for matrix operations.Challenges: Requires NVIDIA GPU, CUDA, cuDNN, and compatible drivers.Setup:Hardware: Ensure an NVIDIA GPU with CUDA support (compute capability â‰¥3.5).Drivers: Install NVIDIA GPU drivers (check compatibility with CUDA version).CUDA and cuDNN: Install CUDA Toolkit and cuDNN for GPU acceleration.Verification: Check GPU availability:import torch
print(torch.cuda.is_available())  # Should return True
print(torch.cuda.get_device_name(0))  # E.g., 'NVIDIA GeForce RTX 3060'2.3 Best Practices for CPU/GPU CompatibilityDynamic Device Selection: Write code to detect and switch between CPU and GPU:import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
input_data = input_data.to(device)Mixed Precision: Use mixed precision (FP16/BF16) on GPUs to reduce memory usage and speed up inference. Example with PyTorch:from torch.cuda.amp import autocast
with autocast():
    output = model(input_data)Gradient Checkpointing: For large models, use gradient checkpointing to trade computation for memory, especially on GPUs with limited VRAM.3. Managing Python and Module VersionsConsistency in Python and module versions is critical to avoid compatibility issues.3.1 Python VersionRecommended Versions: PyTorch and TensorFlow support Python 3.9â€“3.12 (as of 2025). Python 3.11 is a stable choice for compatibility.Installation:Use a package manager like Anaconda or Miniconda:conda create --name ml_env python=3.11
conda activate ml_envOr install via system package manager:sudo apt-get install python3.11 python3.11-dev python3.11-venvVerification:python --version3.2 Module VersionsPyTorch:Latest stable version (as of 2025): 2.6.0Install with CUDA support (e.g., CUDA 12.4):pip install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124Verify installation:import torch
print(torch.__version__)  # Should print 2.6.0
print(torch.cuda.is_available())  # Should print True for GPU setupTensorFlow:Latest stable version (as of 2025): 2.18.0Install with GPU support:pip install tensorflow==2.18.0Verify installation:import tensorflow as tf
print(tf.__version__)  # Should print 2.18.0
print(len(tf.config.list_physical_devices('GPU')))  # Should print >0 for GPUOther Dependencies:Common libraries: numpy, scipy, scikit-learn, pandas, transformers (for NLP models).Example requirements.txt:torch==2.6.0
torchvision==0.18.1
tensorflow==2.18.0
numpy==1.26.4
scipy==1.13.1
scikit-learn==1.5.1
pandas==2.2.2
transformers==4.41.23.3 Best PracticesVirtual Environments: Use conda or venv to isolate dependencies:python -m venv ml_env
source ml_env/bin/activatePin Versions: Always specify exact versions in requirements.txt to ensure reproducibility.Check Compatibility: Verify library compatibility with Python and CUDA versions (e.g., PyTorch 2.6.0 supports CUDA 12.4, TensorFlow 2.18.0 supports CUDA 12.2).4. PyTorch and TensorFlow SetupBoth PyTorch and TensorFlow are popular ML frameworks with GPU support via CUDA.4.1 PyTorchOverview: PyTorch is known for its dynamic computation graph, making it ideal for research and flexible model development.Installation:CPU-only:pip install torch==2.6.0GPU (CUDA 12.4):pip install torch==2.6.0 --index-url https://download.pytorch.org/whl/cu124Model Example (Iris classification):import torch
import torch.nn as nn

class IrisModel(nn.Module):
    def __init__(self):
        super(IrisModel, self).__init__()
        self.fc1 = nn.Linear(4, 10)
        self.fc2 = nn.Linear(10, 3)
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

model = IrisModel().to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))4.2 TensorFlowOverview: TensorFlow is robust for production deployment, with tools like TensorFlow Serving for scalable inference.Installation:CPU-only:pip install tensorflow==2.18.0GPU (CUDA 12.2):pip install tensorflow==2.18.0Model Example (Iris classification):import tensorflow as tf
from tensorflow.keras import layers, models

model = models.Sequential([
    layers.Dense(10, activation='relu', input_shape=(4,)),
    layers.Dense(3, activation='softmax.0')
])
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])4.3 Saving and Loading ModelsPyTorch:# Save
torch.save(model.state_dict(), "iris_model.pth")
# Load
model.load_state_dict(torch.load("iris_model.pth"))
model.eval()TensorFlow:# Save
model.save("iris_model.h5")
# Load
model = tf.keras.models.load_model("iris_model.h5")5. Docker Images for ML DeploymentDocker containers ensure consistent environments across development and production.5.1 Prebuilt Docker ImagesPyTorch:Official images on Docker Hub: pytorch/pytorchExample (CUDA 12.4, Python 3.11):docker pull pytorch/pytorch:2.4.0-cuda12.4-cudnn9-runtimeVariants: runtime (for pre-built apps), devel (for development with compilers).TensorFlow:Official images on Docker Hub: tensorflow/tensorflowExample (GPU, latest release):docker pull tensorflow/tensorflow:latest-gpuVariants: latest-gpu, latest-gpu-jupyter, devel-gpu.AWS Deep Learning Containers:Prebuilt images for PyTorch and TensorFlow optimized for AWS.Example:docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-ec2Use with Amazon ECS, EKS, or SageMaker.5.2 Custom Docker ImageCreate a custom Dockerfile for specific requirements:FROM nvidia/cuda:12.4.0-runtime-ubuntu22.04

# Set environment variables
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHON_VERSION=3.11

# Install system dependencies
RUN apt-get update && apt-get install -y \
    python3-pip python3-dev git \
    && rm -rf /var/lib/apt/lists/*

# Upgrade pip and install dependencies
RUN pip3 install --upgrade pip
COPY requirements.txt .
RUN pip3 install -r requirements.txt

# Install PyTorch and TensorFlow
RUN pip3 install torch==2.6.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
RUN pip3 install tensorflow==2.18.0

# Copy application code
COPY app/ /app/
WORKDIR /app

# Set entrypoint
ENTRYPOINT ["python3", "main.py"]Build the Image:docker build -t ml-model:latest .Run the Container (GPU support):docker run --gpus all -it --rm ml-model:latest5.3 Best PracticesUse NVIDIA Container Toolkit: Enable GPU support in Docker:sudo apt-get install -y nvidia-container-toolkit
nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart dockerShared Memory: Increase shared memory for multi-GPU setups:docker run --gpus all --shm-size=1g -it --rm ml-model:latestMulti-Stage Builds: Use multi-stage builds to reduce image size.Version Pinning: Specify exact versions in requirements.txt to avoid conflicts.6. CUDA InstallationCUDA is required for GPU acceleration with PyTorch or TensorFlow.6.1 PrerequisitesNVIDIA GPU with compute capability â‰¥3.5.Compatible NVIDIA drivers (check NVIDIAâ€™s CUDA compatibility chart).Ubuntu 20.04/22.04 or compatible OS.6.2 Installation Steps (Ubuntu)Install NVIDIA Drivers:sudo apt-get update
sudo apt-get install -y nvidia-driver-550
nvidia-smi  # Verify driver installationInstall CUDA Toolkit (e.g., CUDA 12.4):wget https://developer.download.nvidia.com/compute/cuda/12.4.0/local_installers/cuda_12.4.0_550.54.14_linux.run
sudo sh cuda_12.4.0_550.54.14_linux.runFollow prompts to install CUDA Toolkit (skip driver installation if already done).Install cuDNN:Download cuDNN from NVIDIA Developer (requires account).Example for cuDNN 9:tar -xvf cudnn-linux-x86_64-9.x.x.x_cuda12.x-archive.tar.xz
sudo cp cudnn-*-archive/include/* /usr/local/cuda/include/
sudo cp cudnn-*-archive/lib/* /usr/local/cuda/lib64/Set Environment Variables:echo 'export PATH=/usr/local/cuda-12.4/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrcVerify Installation:nvcc --version  # Should show CUDA 12.46.3 Using Prebuilt Docker ImagesTo avoid manual CUDA installation, use NVIDIAâ€™s CUDA base images (e.g., nvidia/cuda:12.4.0-runtime-ubuntu22.04), which include CUDA and cuDNN.7. Deploying Using Docker and AWSAWS provides multiple services for deploying Dockerized ML models, such as Amazon ECS, EKS, SageMaker, and Lambda. This section focuses on ECS and SageMaker for flexibility and scalability.7.1 Preparing the Docker ImageEnsure your Docker image includes:Trained model artifacts.Inference script (e.g., main.py for FastAPI or TorchServe).Dependencies in requirements.txt.Example inference script (main.py) using FastAPI for PyTorch:from fastapi import FastAPI
import torch
import numpy as np

app = FastAPI()
model = IrisModel()
model.load_state_dict(torch.load("iris_model.pth"))
model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

@app.post("/predict")
async def predict(data: list):
    input_data = torch.tensor(data, dtype=torch.float32).to(device)
    with torch.no_grad():
        output = model(input_data)
    return {"predictions": output.cpu().numpy().tolist()}7.2 Building and Pushing to Amazon ECRCreate an ECR Repository:aws ecr create-repository --repository-name ml-model --region us-east-1Authenticate Docker to ECR:aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account-id>.dkr.ecr.us-east-1.amazonaws.comTag and Push the Image:docker tag ml-model:latest <account-id>.dkr.ecr.us-east-1.amazonaws.com/ml-model:latest
docker push <account-id>.dkr.ecr.us-east-1.amazonaws.com/ml-model:latest7.3 Deploying on Amazon ECSCreate an ECS Cluster:Use AWS Management Console or CLI:aws ecs create-cluster --cluster-name ml-clusterDefine a Task Definition:Create a JSON file (task-definition.json):{
    "family": "ml-task",
    "containerDefinitions": [
        {
            "name": "ml-container",
            "image": "<account-id>.dkr.ecr.us-east-1.amazonaws.com/ml-model:latest",
            "memory": 8000,
            "cpu": 2048,
            "portMappings": [
                {
                    "containerPort": 8080,
                    "hostPort": 8080
                }
            ],
            "essential": true,
            "environment": [
                {"name": "SAGEMAKER_PROGRAM", "value": "main.py"}
            ],
            "resourceRequirements": [
                {
                    "type": "GPU",
                    "value": "1"
                }
            ]
        }
    ],
    "requiresCompatibilities": ["FARGATE"],
    "networkMode": "awsvpc",
    "memory": "16384",
    "cpu": "4096"
}Register the task:aws ecs register-task-definition --cli-input-json file://task-definition.jsonCreate a Service:aws ecs create-service --cluster ml-cluster --service-name ml-service --task-definition ml-task --desired-count 1 --launch-type FARGATE --network-configuration "awsvpcConfiguration={subnets=[subnet-xxxx],securityGroups=[sg-xxxx],assignPublicIp=ENABLED}"Access the Endpoint:Assign a public IP or use an Application Load Balancer to access the FastAPI endpoint (e.g., http://<public-ip>:8080/predict).7.4 Deploying on Amazon SageMakerPrepare Model Artifacts:Package model and inference script in a ascendedUpload to S3:aws s3 cp model.tar.gz s3://<bucket-name>/model.tar.gzCreate a SageMaker Endpoint:Use the SageMaker Python SDK:import sagemaker
from sagemaker.pytorch import PyTorchModel

session = sagemaker.Session()
model = PyTorchModel(
    model_data="s3://<bucket-name>/model.tar.gz",
    role="arn:aws:iam::<account-id>:role/SageMakerRole",
    entry_point="main.py",
    framework_version="2.6.0",
    py_version="py311"
)
predictor = model.deploy(instance_type="ml.g4dn.xlarge", initial_instance_count=1)Test the Endpoint:Send a request:import boto3
client = boto3.client("sagemaker-runtime", region_name="us-east-1")
response = client.invoke_endpoint(
    EndpointName=predictor.endpoint_name,
    ContentType="application/json",
    Body=json.dumps({"data": [[5.1, 3.5, 1.4, 0.2]]})
)
print(response["Body"].read())7.5 Best PracticesUse AWS Deep Learning Containers: Prebuilt images optimized for AWS (e.g., 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.6.0-gpu-py312-cu124-ubuntu22.04-ec2).Optimize Instance Types:CPU: c5.xlarge or t3.xlarge.GPU: g4dn.xlarge (NVIDIA T4) or p4d.24xlarge (NVIDIA A100).Autoscaling: Configure ECS or SageMaker autoscaling to handle load spikes.Monitoring: Use Amazon CloudWatch for logs and performance metrics.8. Additional ConsiderationsModel Optimization:Use TensorRT for optimized GPU inference.Convert models to ONNX for cross-framework compatibility.Security:Use IAM roles with least privilege for ECS/SageMaker.Encrypt model artifacts in S3 using KMS.Cost Management:Use AWS Spot Instances for training to reduce costs.Monitor usage with AWS Cost Explorer.Logging:Integrate logging with FastAPI or TensorFlow Serving:import logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
logger.info("Prediction request received")Testing:Test the endpoint locally using Docker:docker run -p 8080:8080 ml-model:latest
curl -X POST http://localhost:8080/predict -H "Content-Type: application/json" -d '{"data": [[5.1, 3.5, 1.4, 0.2]]}'



